{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "* PEFT technique: [`LoRA`](https://huggingface.co/docs/peft/conceptual_guides/adapter#low-rank-adaptation-lora)\n",
    "* Model: [`gpt2`](https://huggingface.co/openai-community/gpt2)\n",
    "* Evaluation approach: Hugging Face's [`Trainer.evaluate`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.evaluate)\n",
    "* Fine-tuning dataset: [`imdb`](https://huggingface.co/datasets/imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b0f98f-aeb1-4b8f-bb8d-f1aba93e4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display_markdown, Markdown\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, GPT2Tokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700c429-e930-49cb-b0bd-b901ea257662",
   "metadata": {},
   "source": [
    "I've chosen to use Hugging Face's [`evaluate`](https://huggingface.co/docs/evaluate/index) library, which may or may not be included on Udacity's workspace. We'll try to import it, and if it fails, we'll install it using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad2503d-486d-4da8-9148-720ae2980c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import evaluate\n",
    "except ImportError:\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"evaluate\", \"scikit-learn\"])\n",
    "    display_markdown(Markdown('<div class=\"alert alert-block alert-warning\">New depencies were installed dynamically. <span style=\"font-weight: bold;\">You should restart the kernel</span>.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64443e-fa5f-40bb-9be9-7ff03e502485",
   "metadata": {},
   "source": [
    "## Choosing PyTorch's device based on available backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f095b3d-d9d6-44b2-802f-5772274f050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e53fa4b-93d7-4f41-915e-bf923586c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div class=\"alert alert-block alert-success\">Using MPS backend</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    pytorch_device = torch.device(\"cuda\")\n",
    "    display_markdown(Markdown('<div class=\"alert alert-block alert-success\">Using CUDA backend</div>'))\n",
    "elif torch.backends.mps.is_available():\n",
    "    pytorch_device = torch.device(\"mps\")\n",
    "    display_markdown(Markdown('<div class=\"alert alert-block alert-success\">Using MPS backend</div>'))\n",
    "else:\n",
    "    display_markdown(Markdown('<div class=\"alert alert-block alert-warning\">Using CPU backend</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36c831f-f0dc-4e2d-9a23-1ecf867c8543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Pre-Trained Model: `gpt2`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_model_name = \"gpt2\"\n",
    "display(Markdown(f\"### Pre-Trained Model: `{pretrained_model_name}`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55fbdf5c-70eb-4d3f-8ae0-8c11ca7b065f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Metric `accuracy`:\n",
       "\n",
       "```\n",
       "EvaluationModule(name: \"accuracy\", module_type: \"metric\", features: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)}, usage: \"\"\"\n",
       "Args:\n",
       "    predictions (`list` of `int`): Predicted labels.\n",
       "    references (`list` of `int`): Ground truth labels.\n",
       "    normalize (`boolean`): If set to False, returns the number of correctly classified samples. Otherwise, returns the fraction of correctly classified samples. Defaults to True.\n",
       "    sample_weight (`list` of `float`): Sample weights Defaults to None.\n",
       "\n",
       "Returns:\n",
       "    accuracy (`float` or `int`): Accuracy score. Minimum possible value is 0. Maximum possible value is 1.0, or the number of examples input, if `normalize` is set to `True`.. A higher score means higher accuracy.\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1-A simple example\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.5}\n",
       "\n",
       "    Example 2-The same as Example 1, except with `normalize` set to `False`.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\n",
       "        >>> print(results)\n",
       "        {'accuracy': 3.0}\n",
       "\n",
       "    Example 3-The same as Example 1, except with `sample_weight` set.\n",
       "        >>> accuracy_metric = evaluate.load(\"accuracy\")\n",
       "        >>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\n",
       "        >>> print(results)\n",
       "        {'accuracy': 0.8778625954198473}\n",
       "\"\"\", stored examples: 0)```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric_name = \"accuracy\"\n",
    "metric = evaluate.load(metric_name)\n",
    "display_markdown(Markdown(f\"### Metric `{metric_name}`:\\n\\n```\\n{metric}```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d49f0d-49ea-4b9b-84d3-890ec45316ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # Taken from https://huggingface.co/docs/evaluate/transformers_integrations#trainer\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83794da2-39ed-4a9a-9649-fc22c24bb50a",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62172660-f201-42cc-85ef-bcc6d47067f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_dataset(\"imdb\", split=(\"train\", \"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12845</th>\n",
       "      <td>A critical and financial flop when first relea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>Yes, this movie make me feel real horror, when...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13261</th>\n",
       "      <td>A have a female friend who is currently being ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>Of all movies (and I'm a film graduate, if tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22425</th>\n",
       "      <td>Reviewed at the World Premiere screening Sept....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "12845  A critical and financial flop when first relea...      1\n",
       "1013   Yes, this movie make me feel real horror, when...      0\n",
       "13261  A have a female friend who is currently being ...      1\n",
       "1311   Of all movies (and I'm a film graduate, if tha...      0\n",
       "22425  Reviewed at the World Premiere screening Sept....      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_pandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22820</th>\n",
       "      <td>since the plot like Vertigo or Brian DePalma's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>This film is great - well written and very ent...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>A group of seven people fear they are the only...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21362</th>\n",
       "      <td>\"Eighteen\" (2004) tells the story of Pip Ander...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>I had never heard of Dead Man's Bounty when I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "22820  since the plot like Vertigo or Brian DePalma's...      1\n",
       "14378  This film is great - well written and very ent...      1\n",
       "8308   A group of seven people fear they are the only...      0\n",
       "21362  \"Eighteen\" (2004) tells the story of Pip Ander...      1\n",
       "4753   I had never heard of Dead Man's Bounty when I ...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.to_pandas().sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebe9ad-476e-4002-8a97-26ba0bb981c1",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30be9e25-6be5-4965-a082-2f6f322a81fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "display_markdown(Markdown(f\"```\\n{tokenizer}```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6765a5-ccf1-4e68-9a65-bd7c33a8453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Taken from https://huggingface.co/docs/evaluate/transformers_integrations\n",
    "    # No padding is set as suggested by https://huggingface.co/docs/transformers/tasks/sequence_classification#preprocess\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "249620f9-c6a7-4465-8fda-b1374657876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a268b067728e45b99b708603cc5d19ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# I'm using a small test dataset\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True).shuffle(seed=1024).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c63b899-1a5d-4326-bb26-95410a43bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewSentiment(Enum):\n",
    "    NEGATIVE = 0\n",
    "    POSITIVE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {v.value: v.name for v in ReviewSentiment}\n",
    "label2id = {v.name: v.value for v in ReviewSentiment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name,\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(pytorch_device)\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"```\\n{model}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04eed2-0a2c-420a-bfbd-2731c516acea",
   "metadata": {},
   "source": [
    "### Evaluate pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c06b231-5bee-4365-97d1-e033f0b44785",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"nd608-gpt2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ec50db8-f877-4e8c-8fda-6bbe955392cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, padding=\"max_length\"),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331dbc9b-903b-4972-9e21-c81712615bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/250 05:45 < 00:05, 0.71 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
